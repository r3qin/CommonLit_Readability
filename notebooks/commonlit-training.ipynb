{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport re\nimport math\n\nfrom collections import Counter\nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import TensorDataset, DataLoader\n\nfrom transformers import RobertaModel, RobertaTokenizer, BertTokenizer, BertModel, AdamW","metadata":{"execution":{"iopub.status.busy":"2021-06-24T20:16:25.881632Z","iopub.execute_input":"2021-06-24T20:16:25.882012Z","iopub.status.idle":"2021-06-24T20:16:33.052943Z","shell.execute_reply.started":"2021-06-24T20:16:25.881935Z","shell.execute_reply":"2021-06-24T20:16:33.052165Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\ntrain = train[['excerpt', 'target']]\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-24T20:16:42.812652Z","iopub.execute_input":"2021-06-24T20:16:42.813102Z","iopub.status.idle":"2021-06-24T20:16:42.925831Z","shell.execute_reply.started":"2021-06-24T20:16:42.813056Z","shell.execute_reply":"2021-06-24T20:16:42.924986Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"                                             excerpt    target\n0  When the young people returned to the ballroom... -0.340259\n1  All through dinner time, Mrs. Fayre was somewh... -0.315372\n2  As Roger had predicted, the snow departed as q... -0.580118\n3  And outside before the palace a great garden w... -1.054013\n4  Once upon a time there were Three Bears who li...  0.247197","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>excerpt</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>When the young people returned to the ballroom...</td>\n      <td>-0.340259</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n      <td>-0.315372</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>As Roger had predicted, the snow departed as q...</td>\n      <td>-0.580118</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>And outside before the palace a great garden w...</td>\n      <td>-1.054013</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Once upon a time there were Three Bears who li...</td>\n      <td>0.247197</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\ntest = test[['id', 'excerpt']]\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-24T20:16:43.644864Z","iopub.execute_input":"2021-06-24T20:16:43.645206Z","iopub.status.idle":"2021-06-24T20:16:43.664263Z","shell.execute_reply.started":"2021-06-24T20:16:43.645175Z","shell.execute_reply":"2021-06-24T20:16:43.663379Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"          id                                            excerpt\n0  c0f722661  My hope lay in Jack's promise that he would ke...\n1  f0953f0a5  Dotty continued to go to Mrs. Gray's every nig...\n2  0df072751  It was a bright and cheerful scene that greete...\n3  04caf4e0c  Cell division is the process by which a parent...\n4  0e63f8bea  Debugging is the process of finding and resolv...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>excerpt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>c0f722661</td>\n      <td>My hope lay in Jack's promise that he would ke...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>f0953f0a5</td>\n      <td>Dotty continued to go to Mrs. Gray's every nig...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0df072751</td>\n      <td>It was a bright and cheerful scene that greete...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>04caf4e0c</td>\n      <td>Cell division is the process by which a parent...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0e63f8bea</td>\n      <td>Debugging is the process of finding and resolv...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Preprocessing","metadata":{}},{"cell_type":"code","source":"train['excerpt'] = train.excerpt.apply(lambda x: re.sub(r'[\\n]', ' ', x))\ntest['excerpt'] = test.excerpt.apply(lambda x: re.sub(r'[\\n]', ' ', x))","metadata":{"execution":{"iopub.status.busy":"2021-06-24T20:16:44.612115Z","iopub.execute_input":"2021-06-24T20:16:44.612426Z","iopub.status.idle":"2021-06-24T20:16:44.628910Z","shell.execute_reply.started":"2021-06-24T20:16:44.612397Z","shell.execute_reply":"2021-06-24T20:16:44.628108Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Shuffle data\ntrain = train.sample(frac=1).reset_index()\n\n# Train and validation split\nsplit = 4*len(train)//5\nX_train = train.excerpt.iloc[:split]\nX_val = train.excerpt.iloc[split:]\ny_train = train.target.iloc[:split]\ny_val = train.target.iloc[split:]\n\nX_train = X_train.to_numpy()\ny_train = y_train.to_numpy()\nX_val = X_val.to_numpy()\ny_val = y_val.to_numpy()\nX_test = test.excerpt.to_numpy()","metadata":{"execution":{"iopub.status.busy":"2021-06-24T20:16:44.981461Z","iopub.execute_input":"2021-06-24T20:16:44.981798Z","iopub.status.idle":"2021-06-24T20:16:44.990648Z","shell.execute_reply.started":"2021-06-24T20:16:44.981765Z","shell.execute_reply":"2021-06-24T20:16:44.989705Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Pre-trained model downloads","metadata":{}},{"cell_type":"markdown","source":"* Bert","metadata":{}},{"cell_type":"code","source":"# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n# bt = BertModel.from_pretrained('bert-base-uncased')\n# torch.save(bt, \"./bt\")\n# torch.save(tokenizer, \"./tokenizer\")","metadata":{"execution":{"iopub.status.busy":"2021-06-24T20:16:46.014458Z","iopub.execute_input":"2021-06-24T20:16:46.014825Z","iopub.status.idle":"2021-06-24T20:16:46.020185Z","shell.execute_reply.started":"2021-06-24T20:16:46.014789Z","shell.execute_reply":"2021-06-24T20:16:46.019060Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"* RoBerta","metadata":{}},{"cell_type":"code","source":"# tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n# bt = RobertaModel.from_pretrained('roberta-base')\n# torch.save(bt, \"./bt\")\n# torch.save(tokenizer, \"./tokenizer\")","metadata":{"execution":{"iopub.status.busy":"2021-06-24T20:16:46.586069Z","iopub.execute_input":"2021-06-24T20:16:46.586395Z","iopub.status.idle":"2021-06-24T20:16:46.589801Z","shell.execute_reply.started":"2021-06-24T20:16:46.586366Z","shell.execute_reply":"2021-06-24T20:16:46.588877Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Tokenizing","metadata":{}},{"cell_type":"code","source":"tokenizer = torch.load(\"../input/roberta/tokenizer\")\n\nMAX_SEQ_LEN = 256\ncheck_len = tokenizer(X_train.tolist())\ncheck_len_seq = pd.Series(check_len['input_ids']).apply(len).value_counts().sort_index()\nseq_trimmed = check_len_seq[check_len_seq.index > MAX_SEQ_LEN]\nprint(\"If set max length of sequences %d, %d questions will be trimmed\" % \\\n      (MAX_SEQ_LEN, seq_trimmed.sum()))\n\nprint(\"If set max length of sequences %d, %.2f%% questions will be trimmed\" % \\\n      (MAX_SEQ_LEN, seq_trimmed.sum()*100/train.shape[0]))","metadata":{"execution":{"iopub.status.busy":"2021-06-24T20:16:47.226849Z","iopub.execute_input":"2021-06-24T20:16:47.227233Z","iopub.status.idle":"2021-06-24T20:16:51.995260Z","shell.execute_reply.started":"2021-06-24T20:16:47.227199Z","shell.execute_reply":"2021-06-24T20:16:51.994212Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"If set max length of sequences 256, 111 questions will be trimmed\nIf set max length of sequences 256, 3.92% questions will be trimmed\n","output_type":"stream"}]},{"cell_type":"code","source":"X_train_batch = tokenizer(X_train.tolist(), padding='max_length', truncation=True, max_length=MAX_SEQ_LEN, return_token_type_ids=True, return_tensors=\"pt\")\nX_val_batch = tokenizer(X_val.tolist(), padding='max_length', truncation=True, max_length=MAX_SEQ_LEN, return_token_type_ids=True, return_tensors=\"pt\")","metadata":{"execution":{"iopub.status.busy":"2021-06-24T20:16:51.997278Z","iopub.execute_input":"2021-06-24T20:16:51.997940Z","iopub.status.idle":"2021-06-24T20:16:55.663673Z","shell.execute_reply.started":"2021-06-24T20:16:51.997897Z","shell.execute_reply":"2021-06-24T20:16:55.662847Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class MyDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\ntrain_dataset = MyDataset(X_train_batch, y_train)\nval_dataset = MyDataset(X_val_batch, y_val)","metadata":{"execution":{"iopub.status.busy":"2021-06-24T20:16:55.665177Z","iopub.execute_input":"2021-06-24T20:16:55.665502Z","iopub.status.idle":"2021-06-24T20:16:55.673877Z","shell.execute_reply.started":"2021-06-24T20:16:55.665468Z","shell.execute_reply":"2021-06-24T20:16:55.672527Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Modeling","metadata":{}},{"cell_type":"code","source":"# class MyModel(nn.Module):\n\n#     def __init__(self):\n#         super().__init__()\n        \n#         self.bert = torch.load(\"../input/modelused/bt\")\n#         self.fc1 = nn.Linear(MAX_SEQ_LEN*768, 512)\n#         self.fc2 = nn.Linear(512, 128)\n#         self.fc3 = nn.Linear(128, 1)\n#         self.do = nn.Dropout(0.1)\n\n#     def forward(self, input_ids, attention_mask,token_type_ids):\n#         x = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n#         x = torch.tanh(self.fc1(x.last_hidden_state.flatten(start_dim=1, end_dim=-1)))\n#         x = self.do(x)\n#         x = torch.tanh(self.fc2(x))\n#         x = self.do(x)\n#         x = self.fc3(x)\n#         return x\n\n\nclass MyModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        \n        self.bert = torch.load(\"../input/roberta/bt\")\n        self.fc1 = nn.Linear(768, 512)\n        self.fc2 = nn.Linear(512, 128)\n        self.fc3 = nn.Linear(128, 1)\n        self.do = nn.Dropout(0.1)\n\n    def forward(self, input_ids, attention_mask,token_type_ids):\n        x = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n        x = torch.tanh(self.fc1(x.pooler_output))\n        x = self.do(x)\n        x = torch.tanh(self.fc2(x))\n        x = self.do(x)\n        x = self.fc3(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-06-24T20:16:55.675540Z","iopub.execute_input":"2021-06-24T20:16:55.676304Z","iopub.status.idle":"2021-06-24T20:16:55.685472Z","shell.execute_reply.started":"2021-06-24T20:16:55.676259Z","shell.execute_reply":"2021-06-24T20:16:55.684499Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## Training and testing","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\nmodel = MyModel()\nmodel.to(device)\n\nloss_f =  nn.MSELoss()\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=True)\n\noptim = AdamW(model.parameters(), lr=2e-5, weight_decay=0.9)\nEPOCH = 10\n\ndef training_loop(epochs, optimizer, model, loss_f, train_loader, fold_num):\n    \n    best_val_loss = 9999\n    step = 0\n    for epoch in tqdm(range(epochs)):\n        \n        loss_train = 0.0\n        for batch in train_loader:\n            optimizer.zero_grad()\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            token_type_ids = batch['token_type_ids'].to(device)\n            labels = batch['labels'].to(device)\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids).to(device)\n            loss = loss_f(outputs.flatten(), labels.type(torch.float32))\n            loss.backward()\n            optim.step()\n            loss_train += loss.item()\n            \n            step += 1\n            \n            if step % 10 == 0:\n                loss_val = 0.0\n                for batch in val_loader:\n                    input_ids = batch['input_ids'].to(device)\n                    attention_mask = batch['attention_mask'].to(device)\n                    token_type_ids = batch['token_type_ids'].to(device)\n                    labels = batch['labels'].to(device)\n                    outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids).to(device)\n                    loss = loss_f(outputs.flatten(), labels.type(torch.float32))\n                    loss_val += loss.item()\n\n                # print(\"Epoch\", epoch, \"Training loss\", math.sqrt(loss_train/len(train_loader)), \"Validation loss\", math.sqrt(loss_val/len(val_loader)))\n\n                if loss_val < best_val_loss:\n                    best_val_loss = loss_val\n                    model_path = \"./model_%s\" % fold_num\n                    torch.save(model.state_dict(), model_path)\n                    print(\"Epoch\", epoch, \"Training loss\", math.sqrt(loss_train/len(train_loader)), \"Validation loss\", math.sqrt(loss_val/len(val_loader)))\n\n# training_loop(EPOCH, optim, model, loss_f, train_loader, 999)\n    \n\n# preds = []\n# for i in test.excerpt:\n#     test_token = tokenizer([i], padding='max_length', truncation=True, max_length=MAX_SEQ_LEN, return_tensors=\"pt\")\n#     input_ids = test_token['input_ids'].to(device)\n#     attention_mask = test_token['attention_mask'].to(device)\n#     token_type_ids = test_token['token_type_ids'].to(device)\n#     test_pred = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n#     test_pred = test_pred.cpu().detach().numpy().astype(\"float\").item()\n#     preds.append(test_pred)","metadata":{"execution":{"iopub.status.busy":"2021-06-24T20:16:55.688808Z","iopub.execute_input":"2021-06-24T20:16:55.689171Z","iopub.status.idle":"2021-06-24T20:17:09.287781Z","shell.execute_reply.started":"2021-06-24T20:16:55.689134Z","shell.execute_reply":"2021-06-24T20:17:09.286835Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## K Fold","metadata":{}},{"cell_type":"code","source":"num_folds = 5\nkf = KFold(n_splits=num_folds)\n\nkf.get_n_splits(X = train.excerpt, y = train.target)\npreds_folds = []\nfor f, (train_index, val_index) in enumerate(kf.split(X = train.excerpt, y = train.target)):\n    print(\"Fold\", f)\n    X_train = train.excerpt.iloc[train_index].to_numpy()\n    X_val = train.excerpt.iloc[val_index].to_numpy()\n    y_train = train.target.iloc[train_index].to_numpy()\n    y_val = train.target.iloc[val_index].to_numpy()\n    \n    X_train_batch = tokenizer(X_train.tolist(), padding='max_length', truncation=True, max_length=MAX_SEQ_LEN, return_token_type_ids=True, return_tensors=\"pt\")\n    X_val_batch = tokenizer(X_val.tolist(), padding='max_length', truncation=True, max_length=MAX_SEQ_LEN, return_token_type_ids=True, return_tensors=\"pt\")\n    \n    train_dataset = MyDataset(X_train_batch, y_train)\n    val_dataset = MyDataset(X_val_batch, y_val)\n    \n    model = MyModel()\n    model.to(device)\n\n    loss_f =  nn.MSELoss()\n    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=True)\n    optim = AdamW(model.parameters(), lr=2e-5, weight_decay=0.9)\n    EPOCH = 10\n    \n    training_loop(EPOCH, optim, model, loss_f, train_loader, f)\n    \n    preds = []\n    for i in test.excerpt:\n        test_token = tokenizer([i], padding='max_length', truncation=True, max_length=MAX_SEQ_LEN, return_token_type_ids=True, return_tensors=\"pt\")\n        input_ids = test_token['input_ids'].to(device)\n        attention_mask = test_token['attention_mask'].to(device)\n        token_type_ids = test_token['token_type_ids'].to(device)\n        test_pred = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n        test_pred = test_pred.cpu().detach().numpy().astype(\"float\").item()\n        preds.append(test_pred)\n    \n    preds_folds.append(preds)","metadata":{"execution":{"iopub.status.busy":"2021-06-24T20:17:09.289342Z","iopub.execute_input":"2021-06-24T20:17:09.289673Z","iopub.status.idle":"2021-06-24T22:02:49.903767Z","shell.execute_reply.started":"2021-06-24T20:17:09.289638Z","shell.execute_reply":"2021-06-24T22:02:49.901787Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Fold 0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4cfa3b39b54e4ecebda3ad7985dedc6b"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  import sys\n","output_type":"stream"},{"name":"stdout","text":"Epoch 0 Training loss 0.32806010087377596 Validation loss 0.9844775827634784\nEpoch 0 Training loss 0.4174013196480886 Validation loss 0.9583452412307754\nEpoch 0 Training loss 0.48281468914684494 Validation loss 0.8220427161395545\nEpoch 0 Training loss 0.5316041004912815 Validation loss 0.6982417517885213\nEpoch 0 Training loss 0.567925270372262 Validation loss 0.633341348159734\nEpoch 0 Training loss 0.6039244796447842 Validation loss 0.608620258072846\nEpoch 0 Training loss 0.6284186886313922 Validation loss 0.5892119170933868\nEpoch 0 Training loss 0.647614497544729 Validation loss 0.5583561953091473\nEpoch 0 Training loss 0.6885569524657695 Validation loss 0.5371374697032447\nEpoch 0 Training loss 0.7045536609649092 Validation loss 0.5308481224282806\nEpoch 0 Training loss 0.7458886570084187 Validation loss 0.5240588888621318\nEpoch 1 Training loss 0.18609479929795697 Validation loss 0.5213737203708378\nEpoch 1 Training loss 0.22441066448097896 Validation loss 0.49379150032038055\nEpoch 1 Training loss 0.25397905616033806 Validation loss 0.47810368961362126\nEpoch 2 Training loss 0.08161253534273842 Validation loss 0.46437832673233054\nEpoch 2 Training loss 0.16395625851878545 Validation loss 0.4582899490327379\nEpoch 6 Training loss 0.029924509745888794 Validation loss 0.4558780896866149\nEpoch 8 Training loss 0.04561644983174939 Validation loss 0.4540430400423493\nEpoch 9 Training loss 0.040108453922332324 Validation loss 0.4536979264625108\nFold 1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb4f2ea5fc40408794e76de461a642a2"}},"metadata":{}},{"name":"stdout","text":"Epoch 0 Training loss 0.31470784934686963 Validation loss 1.0461158223775735\nEpoch 0 Training loss 0.41695304541188954 Validation loss 1.0092707010735564\nEpoch 0 Training loss 0.48054067216880286 Validation loss 0.8271097974623107\nEpoch 0 Training loss 0.5218382178894945 Validation loss 0.7372878116924753\nEpoch 0 Training loss 0.5529640070840776 Validation loss 0.7320146635237597\nEpoch 0 Training loss 0.5899100245512867 Validation loss 0.6351634508584711\nEpoch 0 Training loss 0.6356143409796469 Validation loss 0.6200176746557379\nEpoch 0 Training loss 0.6563954319310854 Validation loss 0.6042890530938755\nEpoch 0 Training loss 0.6747455111060816 Validation loss 0.5832240172190807\nEpoch 1 Training loss 0.19138775072999648 Validation loss 0.5568738500839151\nEpoch 1 Training loss 0.23011728170324205 Validation loss 0.55676675077749\nEpoch 1 Training loss 0.26404928857394805 Validation loss 0.5504088696732627\nEpoch 1 Training loss 0.29113123200572383 Validation loss 0.5363215571868287\nEpoch 1 Training loss 0.3777226512205309 Validation loss 0.5290972578404707\nEpoch 2 Training loss 0.176752360420662 Validation loss 0.5275027216000677\nEpoch 2 Training loss 0.19999145151696257 Validation loss 0.5243220660014963\nEpoch 6 Training loss 0.09378452531729435 Validation loss 0.5221742693238722\nFold 2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"960b422cd11e4e1caa5fd910c9fcb674"}},"metadata":{}},{"name":"stdout","text":"Epoch 0 Training loss 0.3264594337469085 Validation loss 1.0245432376785661\nEpoch 0 Training loss 0.42640090379542134 Validation loss 0.9523625972552868\nEpoch 0 Training loss 0.48543900218281544 Validation loss 0.7944236837596246\nEpoch 0 Training loss 0.5237052369873453 Validation loss 0.716997385925533\nEpoch 0 Training loss 0.5869973305467878 Validation loss 0.6970376117694086\nEpoch 0 Training loss 0.6168431092939947 Validation loss 0.619887989971374\nEpoch 0 Training loss 0.7024200283493575 Validation loss 0.5935533167123177\nEpoch 0 Training loss 0.738443490630078 Validation loss 0.567420889254689\nEpoch 0 Training loss 0.7532364807047803 Validation loss 0.5573765032432677\nEpoch 1 Training loss 0.1068272303002732 Validation loss 0.5452937263049222\nEpoch 1 Training loss 0.16229787897504874 Validation loss 0.5365390327241384\nEpoch 1 Training loss 0.2606377458604343 Validation loss 0.5183533762476359\nEpoch 1 Training loss 0.4656508100148743 Validation loss 0.5155761780728644\nEpoch 2 Training loss 0.14873740667135932 Validation loss 0.5016086838025793\nEpoch 2 Training loss 0.27285214565727484 Validation loss 0.4987503334217317\nEpoch 2 Training loss 0.32140742630513347 Validation loss 0.4898881419926745\nEpoch 3 Training loss 0.16739815488328552 Validation loss 0.48466011854213437\nEpoch 5 Training loss 0.08385277434366975 Validation loss 0.4824549656774826\nEpoch 5 Training loss 0.13057882264147969 Validation loss 0.4823418489525879\nEpoch 6 Training loss 0.035625229529133176 Validation loss 0.48232208859589304\nEpoch 6 Training loss 0.1154546421208758 Validation loss 0.4817086181763656\nEpoch 6 Training loss 0.14420336119938665 Validation loss 0.47682134891327277\nEpoch 9 Training loss 0.05917205405677362 Validation loss 0.47643565225124046\nFold 3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"791ba24ecc8c4a6f9e7f55ccef64af7e"}},"metadata":{}},{"name":"stdout","text":"Epoch 0 Training loss 0.2939177317177218 Validation loss 1.0756448434853467\nEpoch 0 Training loss 0.3847421636374346 Validation loss 0.9688735882225715\nEpoch 0 Training loss 0.46193617728697517 Validation loss 0.8490817842261913\nEpoch 0 Training loss 0.5050489759292089 Validation loss 0.8152104558290713\nEpoch 0 Training loss 0.5441719526586686 Validation loss 0.7968108485356659\nEpoch 0 Training loss 0.571056818845203 Validation loss 0.7186561312400503\nEpoch 0 Training loss 0.6269860186410265 Validation loss 0.697434856276463\nEpoch 0 Training loss 0.6719335522863685 Validation loss 0.6762108398994311\nEpoch 0 Training loss 0.691124732412198 Validation loss 0.6272464394859214\nEpoch 0 Training loss 0.7284059475466916 Validation loss 0.6000765507153218\nEpoch 0 Training loss 0.7407990398296189 Validation loss 0.5882313016885722\nEpoch 1 Training loss 0.12086597765402207 Validation loss 0.5676029151922076\nEpoch 1 Training loss 0.16909330889543508 Validation loss 0.5548987836910313\nEpoch 1 Training loss 0.21440669643990373 Validation loss 0.5534844175305698\nEpoch 1 Training loss 0.2522013492939382 Validation loss 0.5439216114742313\nEpoch 1 Training loss 0.2785332646243523 Validation loss 0.542335680350507\nEpoch 1 Training loss 0.4848803236703938 Validation loss 0.535916065161847\nEpoch 2 Training loss 0.08187983374670953 Validation loss 0.5324564387763173\nEpoch 2 Training loss 0.13206377961957844 Validation loss 0.5274849361224333\nEpoch 2 Training loss 0.16241854829879693 Validation loss 0.5240381884030151\nEpoch 2 Training loss 0.23848588870923057 Validation loss 0.5092737826219492\nEpoch 2 Training loss 0.2523511530455977 Validation loss 0.5073923259631635\nEpoch 2 Training loss 0.28473277708174016 Validation loss 0.5071179430864372\nEpoch 3 Training loss 0.049712278670251686 Validation loss 0.5063184353489656\nEpoch 3 Training loss 0.08883882278622308 Validation loss 0.5029135171621897\nEpoch 3 Training loss 0.1339229449377356 Validation loss 0.5014919412768503\nEpoch 3 Training loss 0.16517208278257284 Validation loss 0.5014448385799452\nEpoch 4 Training loss 0.17117882475591456 Validation loss 0.4981743356502978\nEpoch 9 Training loss 0.11186741380161272 Validation loss 0.4956592751929841\nEpoch 9 Training loss 0.13566073409364948 Validation loss 0.4932935864621395\nFold 4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f73a62cf74ae4da3a3a30e2487c71d72"}},"metadata":{}},{"name":"stdout","text":"Epoch 0 Training loss 0.3406298677338421 Validation loss 1.0466571030716407\nEpoch 0 Training loss 0.4225961241041979 Validation loss 0.9838855412099905\nEpoch 0 Training loss 0.48739087961027694 Validation loss 0.8579098633869587\nEpoch 0 Training loss 0.5325167617837963 Validation loss 0.7525397798963979\nEpoch 0 Training loss 0.5656757032498787 Validation loss 0.6817057865941731\nEpoch 0 Training loss 0.590458375538171 Validation loss 0.6286837537675187\nEpoch 0 Training loss 0.6394149673280801 Validation loss 0.6033424792970823\nEpoch 0 Training loss 0.6603395180783894 Validation loss 0.5718060717997201\nEpoch 0 Training loss 0.6798729352561831 Validation loss 0.5524818898138948\nEpoch 0 Training loss 0.6932265452786024 Validation loss 0.5442396929607164\nEpoch 0 Training loss 0.7452774140699447 Validation loss 0.5371680545542128\nEpoch 1 Training loss 0.1109764879099735 Validation loss 0.522811049211195\nEpoch 1 Training loss 0.1647481068004914 Validation loss 0.5123148405586576\nEpoch 1 Training loss 0.3751265037225229 Validation loss 0.5037146839950645\nEpoch 2 Training loss 0.1179888103605256 Validation loss 0.4987606074750292\nEpoch 2 Training loss 0.302791008121445 Validation loss 0.4986800762468094\nEpoch 3 Training loss 0.08932300827399595 Validation loss 0.48765715578502955\nEpoch 3 Training loss 0.11353104568763395 Validation loss 0.4850242892069381\nEpoch 9 Training loss 0.051049667940103016 Validation loss 0.48402422676839274\n","output_type":"stream"}]},{"cell_type":"code","source":"tgt = np.mean(np.array(preds_folds), axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:31:23.416476Z","iopub.execute_input":"2021-06-20T00:31:23.41681Z","iopub.status.idle":"2021-06-20T00:31:23.421647Z","shell.execute_reply.started":"2021-06-20T00:31:23.416777Z","shell.execute_reply":"2021-06-20T00:31:23.420675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ans = pd.DataFrame({'id': test.id, 'target': tgt})\nans","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:31:24.512821Z","iopub.execute_input":"2021-06-20T00:31:24.513147Z","iopub.status.idle":"2021-06-20T00:31:24.526469Z","shell.execute_reply.started":"2021-06-20T00:31:24.513114Z","shell.execute_reply":"2021-06-20T00:31:24.525662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ans.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:31:49.156164Z","iopub.execute_input":"2021-06-20T00:31:49.1565Z","iopub.status.idle":"2021-06-20T00:31:49.337765Z","shell.execute_reply.started":"2021-06-20T00:31:49.156469Z","shell.execute_reply":"2021-06-20T00:31:49.337044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}